{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"fods_1.csv\").to_numpy()\n",
    "data = pd.read_csv(\"fods_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = data.sample(frac = 0.8)\n",
    "test_data = data.drop(training_data.index)\n",
    "# np.random.shuffle(data)\n",
    "# N = data.shape[0]\n",
    "\n",
    "training_data = training_data.to_numpy()\n",
    "test_data = test_data.to_numpy()\n",
    "\n",
    "# N_train = ceil(N * 0.8)\n",
    "# N_test = N - N_train\n",
    "\n",
    "X_train = training_data[:, :2].T\n",
    "Y_train = training_data[:,2].T\n",
    "\n",
    "X_test = test_data[:,:2].T\n",
    "Y_test = test_data[:,2].T\n",
    "\n",
    "N_train = len(X_train[0])\n",
    "N_test = len(X_test[0])\n",
    "\n",
    "print(N_train, N_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POLYNOMIAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize (X):\n",
    "\n",
    "    mean = np.array(np.mean(X[1:, :], axis = 1)).reshape([len(X) - 1, 1])\n",
    "    std = np.array(np.std(X[1:, :], axis = 1)).reshape([len(X) - 1, 1])\n",
    "\n",
    "    mean = np.concatenate((np.zeros([1,1]), mean), axis = 0)\n",
    "    std = np.concatenate((np.ones([1,1]), std), axis = 0)\n",
    "\n",
    "    mean = np.repeat(mean, len(X[0]), axis = 1)\n",
    "    std = np.repeat(std, len(X[0]), axis = 1)\n",
    "\n",
    "    X = (X - mean) / std\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_feature_matrix(X, deg):\n",
    "\n",
    "    mat = np.ones([1,len(X[0])])\n",
    "    for i in range (1, deg + 1):\n",
    "        for j in range(i+1):\n",
    "            k = i - j\n",
    "            newRow = np.array((X[0]**j) * (X[1]**k)).reshape([1, len(X[0])])\n",
    "            mat = np.concatenate((mat, newRow), axis = 0)\n",
    "            \n",
    "    return normalize(mat)\n",
    "    # return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(Y, X, W):\n",
    "    return np.sum(1 / (2 * len(X[0])) * np.power((Y - W.T @ X), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(Y, X, W, alpha, iterations):\n",
    "    \n",
    "    N = len(X[0])\n",
    "    cost = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        W = W + (alpha / N)*(X @ (Y - W.T @ X).T)\n",
    "        cost[i] = loss_function(Y, X, W)\n",
    "    \n",
    "    return W, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_descent(Y, X, W, alpha, iterations):\n",
    "\n",
    "    N = len(X[0])\n",
    "    M = len(X)\n",
    "    cost = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        index = random.randint(0, N - 1)\n",
    "        Xi = X[:, index].reshape([M, 1])\n",
    "        Yi = Y[index].reshape([1, 1])\n",
    "\n",
    "        W = W + (alpha / N) * (Xi @ (Yi - W.T @ Xi).T)\n",
    "        cost[i] = loss_function(Y, X, W)\n",
    "\n",
    "    return W, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_polynomial_regression(Y, X, descent_type, alphas):\n",
    "\n",
    "    error_history = np.zeros([10, 1])\n",
    "    W_history = []\n",
    "\n",
    "    # plt.title(\"Polynomial regression\")\n",
    "    # plt.xlabel(\"Iterations\")\n",
    "    # plt.ylabel(\"Error\")\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        count = (i + 1) * (i + 2) / 2\n",
    "        count = int(count)\n",
    "\n",
    "        _X = X[:count, :]\n",
    "        W = np.zeros([count, 1])\n",
    "\n",
    "        iterations = 10000\n",
    "        # alpha_array = []\n",
    "        # final_cost_array = []\n",
    "\n",
    "        # alpha = 0.1\n",
    "\n",
    "        # while(alpha >= 0.001):\n",
    "\n",
    "        #     W_grad, cost_grad = descent_type(Y, _X, W, alpha, iterations)\n",
    "        #     rounded_cost = float(\"{:.4f}\".format(cost_grad[len(cost_grad) - 1]))\n",
    "        #     alpha_array.append(alpha)\n",
    "        #     final_cost_array.append(rounded_cost)\n",
    "\n",
    "        #     alpha -= 1e-3\n",
    "        \n",
    "        # plt.plot(alpha_array, )\n",
    "\n",
    "        W_desc, cost_desc = descent_type(Y, _X, W, alphas[i], iterations)\n",
    "\n",
    "        grad_descent_graph = np.array([i for i in range (len(cost_desc))])\n",
    "\n",
    "        # title_string = \"Polynomial Regression for degree \" + str(i)\n",
    "        rounded_cost = float(\"{:.4f}\".format(cost_desc[len(cost_desc) - 1]))\n",
    "\n",
    "        error_history[i][0] = rounded_cost \n",
    "        # W_history[i] = W_history[i].reshape([1, len(W_desc)])\n",
    "        W_history.append(W_desc.reshape([len(W_desc)]))\n",
    "\n",
    "        plt.plot(grad_descent_graph, cost_desc)\n",
    "        \n",
    "        # plt.show()\n",
    "\n",
    "        # plt.plot(alpha_array, final_cost_array)\n",
    "        # plt.show()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    return error_history, W_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_polynomial_regression(Y, X, W):\n",
    "\n",
    "    error_history = np.zeros([10, 1])\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        count = (i + 1) * (i + 2) / 2\n",
    "        count = int(count)\n",
    "\n",
    "        _X = X[:count, :]\n",
    "        \n",
    "        error_history[i][0] = loss_function(Y, _X, W[i])\n",
    "    \n",
    "    return error_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_feature_matrix(X_train, 9)\n",
    "training_error_gradient, W_gradient = training_polynomial_regression(Y_train, X, gradient_descent, [0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04])\n",
    "training_error_stochastic, W_stochastic = training_polynomial_regression(Y_train, X, stochastic_descent, [0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.1, 0.06, 0.06])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = generate_feature_matrix(X_test, 9)\n",
    "W = [i.reshape([len(i), 1]) for i in W_gradient]\n",
    "testing_error_gradient = testing_polynomial_regression(Y_test, X, W)\n",
    "\n",
    "rounded_test_error = [float(\"{:.3f}\".format(testing_error_gradient[i][0])) for i in range(10)]\n",
    "\n",
    "plt.plot([i for i in range(10)], rounded_test_error, marker = 'o')\n",
    "plt.xticks([i for i  in range(10)])\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Testing Error\")\n",
    "plt.title(\"Gradient Descent Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = [i.reshape([len(i), 1]) for i in W_stochastic]\n",
    "testing_error_stochastic = testing_polynomial_regression(Y_test, X, W)\n",
    "\n",
    "rounded_test_error = [float(\"{:.3f}\".format(testing_error_stochastic[i][0])) for i in range(10)]\n",
    "\n",
    "plt.plot([i for i in range(10)], rounded_test_error, marker = 'o')\n",
    "plt.xticks([i for i  in range(10)])\n",
    "plt.xlabel(\"Degree\")\n",
    "plt.ylabel(\"Testing Error\")\n",
    "plt.title(\"Stochastic Descent Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = np.array([i for i in range(10)]).reshape([10,1])\n",
    "polynomial_regression_result = np.concatenate((degrees, training_error_gradient, training_error_stochastic, testing_error_gradient, testing_error_stochastic), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_table = pd.DataFrame(polynomial_regression_result, columns = ['Degree', 'Training Error (Gradient)', 'Training Error (Stochastic)','Testing Error (Gradient)', 'Testing Error (Stochastic)'])\n",
    "polynomial_regression_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REGULARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_data = data.sample(frac = 0.8)\n",
    "# test_data = data.drop(training_data.index)\n",
    "# # np.random.shuffle(data)\n",
    "# # N = data.shape[0]\n",
    "\n",
    "# training_data = training_data.to_numpy()\n",
    "# test_data = test_data.to_numpy()\n",
    "\n",
    "# # N_train = ceil(N * 0.8)\n",
    "# # N_test = N - N_train\n",
    "\n",
    "# X_train = training_data[:, :2].T\n",
    "# Y_train = training_data[:,2].T\n",
    "\n",
    "# X_test = test_data[:,:2].T\n",
    "# Y_test = test_data[:,2].T\n",
    "\n",
    "# N_train = len(X_train[0])\n",
    "# N_test = len(X_test[0])\n",
    "\n",
    "# print(N_train, N_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_loss_function(Y, X, W, lam, q):\n",
    "\n",
    "    loss1 = np.sum(1 / (2 * len(X[0])) * ((Y - W.T @ X) ** 2))\n",
    "    loss2 = np.sum(lam / 2 * (W ** q))\n",
    "\n",
    "    return loss1 + loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_function(Y, X, W):\n",
    "    return np.sum(1 / (2 * len(X[0])) * ((Y - W.T @ X) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = lambda\n",
    "def regularized_gradient_descent(Y, X, W, q, alpha, l, iterations):\n",
    "    N = len(X[0])\n",
    "    M = len(X)\n",
    "    cost = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        W = W - (alpha) * (((-1 / N) * (X @ (Y - W.T @ X).T)) + ((l / 2) * q * (W ** (q - 1))))\n",
    "        cost[i] = regularized_loss_function(Y, X, W, l, q)\n",
    "\n",
    "    return W, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_regularized_gradient_descent(Y, X, ):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
