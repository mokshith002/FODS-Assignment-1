{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_t : [2.35751945 2.46979863] y_t : 4.701677345537757\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read data from csv file\n",
    "data = pd.read_csv('fods_1.csv')\n",
    "\n",
    "# Convert data to numpy array\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Shuffle and split data\n",
    "np.random.shuffle(data)\n",
    "N = data.shape[0]\n",
    "N_train = ceil(N * 0.8)\n",
    "N_test = N - N_train\n",
    "X_train = data[:N_train,0:2]\n",
    "y_train = data[:N_train,2]\n",
    "X_test = data[N_train:,0:2]\n",
    "y_test = data[N_train:,2]\n",
    "\n",
    "print(f\"X_t : {np.average(X_train, axis = 0)} y_t : {np.average(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost\n",
    "We add a dummy input variable $x_0^{(i)} = 1$ to all $N$ examples in order to handle bias.\n",
    "$$ \n",
    "f_{w}(x^{(i)}) = w_0 x_0^{(i)} + w_1 x_1^{(i)} + \\cdots + w_d x_d^{(i)} \\\\\n",
    "J_{w} = \\frac{1}{2N} \\sum_{i=1}^{N} (f_{w}(x^{(i)}) - y^{(i)})^2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w):\n",
    "  \"\"\"\n",
    "  compute cost\n",
    "  Args:\n",
    "    X : (ndarray): Shape (N,d) matrix of examples with multiple features\n",
    "    w : (ndarray): Shape (d,1)   parameters for prediction   \n",
    "  Returns\n",
    "    cost: (scalar)             cost\n",
    "  \"\"\"\n",
    "  N = X.shape[0]\n",
    "  f_w = X @ w\n",
    "  # cost = (1 / (2 * N)) * np.sum((f_w - y) ** 2)\n",
    "  cost = np.sum((f_w - y) ** 2 / (2 * N))\n",
    "  return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(w,b)}{\\partial w_j}  = \\frac{1}{N} \\sum\\limits_{i = 1}^{N} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}_j \\\\\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w):\n",
    "  \"\"\"\n",
    "  Computes the gradient for linear regression \n",
    "\n",
    "  Args:\n",
    "    X : (array_like Shape (N,d)) variable such as house size \n",
    "    y : (array_like Shape (N,1)) actual value \n",
    "    w : (array_like Shape (d,1)) Values of parameters of the model      \n",
    "  Returns\n",
    "    dj_dw: (array_like Shape (d,1)) The gradient of the cost w.r.t. the parameters w. \n",
    "                                \n",
    "  \"\"\"\n",
    "  N, d = X.shape\n",
    "  f_w = X @ w\n",
    "  # dj_dw = (1 / N) * (X.T @ (f_w - y))\n",
    "  dj_dw = ((X.T / N) @ (f_w - y))\n",
    "  return dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(X, y, w_in, cost_fn, grad_fn, alpha, num_iters):\n",
    "  \"\"\"\n",
    "  Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "  num_iters gradient steps with learning rate alpha\n",
    "\n",
    "  Args:\n",
    "    X : (array_like Shape (N,d)    matrix of examples \n",
    "    y : (array_like Shape (N,))    target value of each example\n",
    "    w_in : (array_like Shape (d,)) Initial values of parameters of the model\n",
    "    cost_fn: function to compute cost\n",
    "    grad_fn: function to compute the gradient\n",
    "    alpha : (float) Learning rate\n",
    "    num_iters : (int) number of iterations to run gradient descent\n",
    "  Returns\n",
    "    w : (array_like Shape (d,)) Updated values of parameters of the model after\n",
    "        running gradient descent\n",
    "  \"\"\"\n",
    "  N, d = X.shape\n",
    "    \n",
    "  # An array to store values at each iteration primarily for graphing later\n",
    "  hist = {}\n",
    "  hist[\"cost\"] = []; hist[\"params\"] = []; hist[\"grads\"] = []; hist[\"iter\"] = [];\n",
    "  \n",
    "  w = deepcopy(w_in)  #avoid modifying global w within function\n",
    "  save_interval = np.ceil(num_iters / 1000) # prevent resource exhaustion for long runs\n",
    "  last = cost_fn(X, y, w)\n",
    "\n",
    "  for i in range(num_iters):\n",
    "\n",
    "      # Calculate the gradient and update the parameters\n",
    "      dj_dw = grad_fn(X, y, w)   \n",
    "\n",
    "      # Update Parameters using w, b, alpha and gradient\n",
    "      w = w - alpha * dj_dw              \n",
    "    \n",
    "      # Save cost J,w,b at each save interval for graphing\n",
    "      if i == 0 or i % save_interval == 0:   \n",
    "          hist[\"cost\"].append(cost_fn(X, y, w))\n",
    "          hist[\"params\"].append([w])\n",
    "          hist[\"grads\"].append([dj_dw])\n",
    "          hist[\"iter\"].append(i)\n",
    "\n",
    "      # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "      if i % ceil(num_iters/10) == 0:\n",
    "          cst = cost_fn(X, y, w)\n",
    "          if cst > last * 10:\n",
    "            print(f\"**Diverging at alpha: {alpha}\")\n",
    "            return w, hist\n",
    "          last = cst\n",
    "          # print(f\"Iteration {i:4d}: Cost {cst:8.2f}   \")\n",
    "          # with np.printoptions(precision=3, suppress=True):\n",
    "          #   print(f\"w: {w}  dj/dw: {dj_dw}\")\n",
    "          \n",
    "      \n",
    "  return w, hist #return w,b and history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree: 0, d: 1\n",
      "Alpha: [1.e-05 1.e-04 1.e-03 1.e-02 1.e-01]\n",
      "Cost: [12.218 10.433  2.878  1.384  1.384]\n",
      "\n",
      "\n",
      "Degree: 1, d: 3\n",
      "Alpha: [1.e-05 1.e-04 1.e-03 1.e-02 1.e-01]\n",
      "Cost: [9.598 1.864 1.09  0.952 0.917]\n",
      "\n",
      "\n",
      "Degree: 2, d: 6\n",
      "Alpha: [1.e-07 1.e-06 1.e-05 1.e-04 1.e-03]\n",
      "Cost: [11.96   8.65   3.202  2.364  1.103]\n",
      "\n",
      "\n",
      "Degree: 3, d: 10\n",
      "**Diverging at alpha: 0.001\n",
      "Alpha: [1.e-07 1.e-06 1.e-05 1.e-04 1.e-03]\n",
      "Cost: [  7.987   5.606   3.974   1.533 224.107]\n",
      "\n",
      "\n",
      "Degree: 4, d: 15\n",
      "**Diverging at alpha: 1e-05\n",
      "**Diverging at alpha: 0.0001\n",
      "Alpha: [1.e-08 1.e-07 1.e-06 1.e-05 1.e-04]\n",
      "Cost: [9.223e+00 7.583e+00 4.620e+00 5.768e+64 2.370e+03]\n",
      "\n",
      "\n",
      "Degree: 5, d: 21\n",
      "**Diverging at alpha: 1.0000000000000002e-06\n",
      "**Diverging at alpha: 1e-05\n",
      "Alpha: [1.e-09 1.e-08 1.e-07 1.e-06 1.e-05]\n",
      "Cost: [   10.365     8.334     5.348   352.084 37795.224]\n",
      "\n",
      "\n",
      "Degree: 6, d: 28\n",
      "**Diverging at alpha: 1.0000000000000002e-08\n",
      "**Diverging at alpha: 1.0000000000000002e-07\n",
      "Alpha: [1.e-11 1.e-10 1.e-09 1.e-08 1.e-07]\n",
      "Cost: [1.171e+001 1.072e+001 7.875e+000 1.817e+222 8.786e+003]\n",
      "\n",
      "\n",
      "Degree: 7, d: 36\n",
      "**Diverging at alpha: 1.0000000000000003e-10\n",
      "**Diverging at alpha: 1.0000000000000003e-09\n",
      "Alpha: [1.e-13 1.e-12 1.e-11 1.e-10 1.e-09]\n",
      "Cost: [1.215e+001 1.179e+001 1.036e+001 6.340e+199 2.703e+003]\n",
      "\n",
      "\n",
      "Degree: 8, d: 45\n",
      "**Diverging at alpha: 1.0000000000000002e-12\n",
      "Alpha: [1.e-16 1.e-15 1.e-14 1.e-13 1.e-12]\n",
      "Cost: [1.232e+001 1.229e+001 1.218e+001 1.163e+001 1.091e+180]\n",
      "\n",
      "\n",
      "Degree: 9, d: 55\n",
      "Alpha: [1.e-19 1.e-18 1.e-17 1.e-16 1.e-15]\n",
      "Cost: [12.424 12.366 12.342 12.308 12.095]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_degree = 10\n",
    "X_cur = np.ones(N_train).reshape(-1, 1)\n",
    "alpha_count = 5\n",
    "alpha_vector = np.zeros(alpha_count)\n",
    "alpha_vector[0] = 1e-5\n",
    "for i in range(1, alpha_count):\n",
    "    alpha_vector[i] = alpha_vector[i-1] * 10\n",
    "num_iters = 1000\n",
    "cost_matrix = np.zeros((max_degree, alpha_count))\n",
    "# print(X_train[:5])\n",
    "for degree in range(max_degree):\n",
    "    if degree > 0:\n",
    "        for d1 in range(degree + 1):\n",
    "            d2 = degree - d1\n",
    "            X1new = (X_train[:,0] ** d1)\n",
    "            X2new = (X_train[:,1] ** d2)\n",
    "            X_cur = np.append(X_cur, (X1new * X2new).reshape(-1, 1), axis=1)\n",
    "    if degree == 2:\n",
    "        alpha_vector /= 100\n",
    "    if degree > 3:\n",
    "        alpha_vector /= 10\n",
    "    if degree > 5:\n",
    "        alpha_vector /= 10\n",
    "    if degree > 7:\n",
    "        alpha_vector /= 10\n",
    "    d = X_cur.shape[1]\n",
    "    print(f\"Degree: {degree}, d: {d}\")\n",
    "    w_in = np.zeros(d)\n",
    "    for a_i in range(alpha_count):\n",
    "        # print(f\"Alpha: {alpha_vector[a_i]}\")\n",
    "        w, hist = grad_descent(X_cur, y_train, w_in, compute_cost, compute_gradient, alpha_vector[a_i], num_iters)\n",
    "        cost_matrix[degree][a_i] = compute_cost(X_cur, y_train, w)\n",
    "        # print(f\"Cost: {cost_matrix[degree][a_i] : .3f}\")\n",
    "        # with np.printoptions(precision=3, suppress=True):\n",
    "        #     print(f\"w: {w}\")\n",
    "    print(f\"Alpha: {alpha_vector}\")\n",
    "    with np.printoptions(precision=3, suppress=True):\n",
    "        print(f\"Cost: {cost_matrix[degree]}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
